{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multi Layer Perceptron\n",
    "\n",
    "> *\"Rome wasn't build in a day\" - Unknown*\n",
    "\n",
    "The MLP represents the simplest architecture that forms the basis for every other complex network architecture found today. In this notebook we are going to implement a MLP form scratch using Pytorch that recognizes handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Every Neural Network must process some data by definition. Pytorch provides a straightforward API for data loading. To define a Dataset you just need to extend the `Dataset` class. \n",
    "Pytorch provides by default some datasets that you can use (including MNIST), but for more pedagogical value we are going to implement it from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 9680k  100 9680k    0     0  4186k      0  0:00:02  0:00:02 --:--:-- 4186k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 28881  100 28881    0     0   114k      0 --:--:-- --:--:-- --:--:--  114k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0^C\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  4542  100  4542    0     0  17469      0 --:--:-- --:--:-- --:--:-- 17469\n",
      "gzip: resources/test-labels already exists; do you wish to overwrite (y or n)? ^C\n"
     ]
    }
   ],
   "source": [
    "# Create a directory to store resources\n",
    "!mkdir -p resources\n",
    "\n",
    "# Download the training set\n",
    "!curl -o resources/train-images.gz http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "!curl -o resources/train-labels.gz http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "\n",
    "# Download the test set\n",
    "!curl -o resources/test-images.gz http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    "!curl -o resources/test-labels.gz http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
    "\n",
    "# Decompress all archives\n",
    "!gzip -d resources/*.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing a custom dataset we need to implement just 2 methods: `__len__` and `__getitem__`. The first one should return the total number of examples on the dataset, while the other one must return a record given its position (as an index).\n",
    "In this example, since the datasets is pretty small, it's possible to load it directly into the memory. In this case, we are going to load it entirely into a one dimensional *tensor* to then change its dimensions (shape) by creating a *view* that matches the right dimensions.\n",
    "\n",
    "\n",
    "`[ p1_1, ... p1_n, p2_1, ...., p2_n, ....] ` -- *view* --> `[[p1_1, .., p1_n], [p2_1, ..., p2_n], ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "def get_uint(b: bytes) -> int:\n",
    "    return int.from_bytes(b, 'big') \n",
    "\n",
    "# Custom dataset that loads the whole dataset into a tensor\n",
    "class CustomMNIST(Dataset):\n",
    "\n",
    "    def __init__(self, images_path: str, labels_path: str, device='cpu'):\n",
    "        print('Loading the dataset...')\n",
    "        with open(images_path, 'rb') as images_file:\n",
    "            images_data = images_file.read()\n",
    "\n",
    "        with open(labels_path, 'rb') as labels_file:\n",
    "            labels_data = labels_file.read()\n",
    "        \n",
    "        # Parse the header. For more information see the Website of MNIST \n",
    "        self.dataset_size = get_uint(images_data[4:8])\n",
    "        self.image_width = get_uint(images_data[8:12])\n",
    "        self.image_height = get_uint(images_data[12:16])\n",
    "\n",
    "        # Define the size of an image in bytes\n",
    "        row_sz = self.image_width * self.image_height\n",
    "        \n",
    "        # Parse the actual data\n",
    "        self.images_data = torch.frombuffer(images_data[16:], dtype=torch.uint8).float().view(-1, row_sz).to(device)\n",
    "        self.labels_data = torch.frombuffer(labels_data[8:], dtype=torch.uint8).to(device)\n",
    "\n",
    "        print(f'Loaded MNIST with {self.dataset_size} ({self.image_width}x{self.image_height}) images')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_size\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images_data[idx], self.labels_data[idx] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test if it works correcly by printing one of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Loaded MNIST with 60000 (28x28) images\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "training_data = CustomMNIST('resources/train-images', 'resources/train-labels')\n",
    "data, label = dataset.__getitem__(5)\n",
    "data = data.reshape(training_data.image_height, training_data.image_width, 1)\n",
    "plt.imshow(data)\n",
    "plt.show()\n",
    "print(f'Label: {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "Every model in PyTorch is defined by creating a subclass of `nn.Module`. Each layer of the network is in turn a module, so in our case we need first to define 2 modules: the linear layer and the activation function layer. The first one is just a matrix multiplication of a weight matrix. The dimensions of such matrix can be passed as parameters. The second is just a ReLU activation, so we just zero out every negative element.\n",
    "The operations that must be done by each layer are implemented inside the `forward` method, which takes as input a tensor coming from the output of the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import math\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super(Linear, self).__init__()\n",
    "        # Weights are initialized using a Uniform distribution bounded on -sqrt(k), where k is 1/no_features\n",
    "        k = math.sqrt(1 / in_features)\n",
    "\n",
    "        # Instead of adding plain pytorch tensors, we add them to Parameters class so that they are considered as model's parameters\n",
    "        self.weights = Parameter(torch.empty(in_features, out_features).uniform_(-k, k))\n",
    "        # add biases\n",
    "        self.bias = Parameter(torch.rand(out_features).uniform_(-k, k))\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "class ReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x[(x < 0)] = 0\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, in the linear layer we just initialized a `(input_features x out_features)` matrix with elements sampled from a uniform distribution bounded at $k = \\sqrt{\\frac{1}{input features}}$. We then return in the forward method the matrix multiplication between the input and such weight matrix. We also add a bias.\n",
    "The ReLU activation is just straightforward since we zero out just the negative elements, returning the result.\n",
    "\n",
    "Now that we have define the layers of our network, we can implement our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, in_features: int, hidden_units: int, out_features: int) -> None:\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            Linear(in_features, hidden_units),\n",
    "            ReLU(),\n",
    "            Linear(hidden_units, hidden_units),\n",
    "            ReLU(),\n",
    "            Linear(hidden_units, out_features),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute each layer sequentially pytorch provides the `Sequential` layer. Thus our network will be simply made of 3 layers (input, hidden, output), thus forcing each input signal to propagate sequentially between each layer in order to reach the output layer.\n",
    "Let's now create the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is a GPU, use it. Then create the model and move it to the GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MultiLayerPerceptron(784, 512, 10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Machine Learning is about training models with data. We defined both the model and our data, so now it's the time to learn our model and see how it will perform on test data alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Loaded MNIST with 60000 (28x28) images\n",
      "Loading the dataset...\n",
      "Loaded MNIST with 10000 (28x28) images\n",
      "Loss: 0.068564  [57600/60000]"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "\n",
    "# Load the datasets and wrap them into dataloaders\n",
    "training_data = CustomMNIST('resources/train-images', 'resources/train-labels', device)\n",
    "test_data = CustomMNIST('resources/test-images', 'resources/test-labels', device)\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Define the optimizer as the Stochastic Gradient Descent\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the training loop\n",
    "def train_loop(dataloader: DataLoader, model: nn.Module, loss_fn) -> None:\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Do a forward pass to compute the loss with respect to the real value\n",
    "        res = model(X)\n",
    "        loss = loss_fn(res, y)\n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            sys.stdout.write(f\"\\rLoss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "# Train the model\n",
    "train_loop(train_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.112232 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we are going to test the model\n",
    "def test_loop(dataloader: DataLoader, model: nn.Module, loss_fn) -> None:\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "test_loop(test_dataloader, model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 - mach-nix-jupyter",
   "language": "python",
   "name": "ipython_mach-nix-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
