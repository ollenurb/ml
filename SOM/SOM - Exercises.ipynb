{"cells":[{"cell_type":"markdown","metadata":{"id":"Uk5wbwMn6cU5"},"source":["# Self Organizing Maps Exercises\n","Here are some proposed exercises from the lessons on Self Organizing Maps of the Neural Networks and Deep Learning course at the University Of Turin.\n","I was too lazy to made them by hand so I opted for using numpy :) "]},{"cell_type":"markdown","metadata":{"id":"C4e-f60m6u4p"},"source":["## 1. Update rule for BMU(s)\n","Given the examples $x$ and the weights $w$, show the updated weights of the BMUs"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":221,"status":"ok","timestamp":1659446149645,"user":{"displayName":"Matteo Brunello","userId":"16731021691105549961"},"user_tz":-120},"id":"Tp_7ccpwv1cL","outputId":"701e7c10-2c90-42cf-d146-58bf9943f584"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1.    1.725]\n","[1.     2.3625]\n","[-1.  -1.5]\n","[-3.   -3.75]\n","[4. 5.]\n","[4. 5.]\n"]}],"source":["import numpy as np\n","\n","w = np.array([\n","    [0, 0],\n","    [0, 1],\n","    [1, 1.45],\n","    [3, 4]              \n","])\n","\n","x = np.array([\n","    [1, 2],\n","    [1, 3],\n","    [-2, -3],\n","    [-5, -6],\n","    [5, 6],\n","    [4, 5]\n","])\n","\n","eta = 0.5\n","for x_i in x:\n","  min_dist = np.linalg.norm(x_i - w, axis = 1)\n","  bmu_index = np.argmin(min_dist)\n","  # Apply the update rule for the BMU\n","  w[bmu_index] = w[bmu_index] + eta * (x_i - w[bmu_index]) \n","  print(w[bmu_index])"]},{"cell_type":"markdown","metadata":{"id":"neHGpm1-oERN"},"source":["## 2.\n"]},{"cell_type":"code","execution_count":98,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1659446985524,"user":{"displayName":"Matteo Brunello","userId":"16731021691105549961"},"user_tz":-120},"id":"CWiJxvWpoDfE","outputId":"79cc026f-2640-4a36-946e-194109c54384"},"outputs":[],"source":["def h(w, bmu, sigma):\n","  # Corresponds to the squared norm (np.linalg.norm(w - bmu, axis = 1) ** 2) \n","  dist = np.sum((w - bmu) ** 2, axis = 2)\n","  return np.exp(-dist / 2 * (sigma ** 2))\n","\n","w = np.array([[\n","    [0, 0, 0],\n","    [1, 1, 1]\n","  ],[\n","    [0, -1, 0],\n","    [-1, -1, -1]\n","  ]])\n","\n","x = np.array([\n","    [2, 2, 2],\n","    [1, 2, 3],\n","    [1, 3, 0]\n","])\n","\n","eta = 0.5\n","sigma = 0.5\n","\n","for x_i in x:\n","  dist = np.linalg.norm(x_i - w, axis = 2)\n","  bmu_index = np.argmin(dist)\n","  # Since Argmin returns a flattened index, we need to convert it into a 2D index\n","  bmu_index = np.unravel_index(bmu_index, dist.shape)\n","  h_vals = h(w, w[bmu_index], sigma)\n","  # Apply the update rule for each weight\n","  # Since the dimensions of h_vals and (x_i - w[bmu_index]) doesn't match we expand h_vals \n","  # with one additional dimension so that we can broadcast\n","  w = w + eta * h_vals[..., None] * (x_i - w[bmu_index])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMbKW76QWfa/xMWxtdBx1FA","name":"SOM - Exercises.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.9"},"vscode":{"interpreter":{"hash":"0c5cd2c897736360451158f80dd1aa70d542ae4f945634860cc00b93c12e0b36"}}},"nbformat":4,"nbformat_minor":0}
